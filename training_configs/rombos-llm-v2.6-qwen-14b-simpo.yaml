# Model arguments
model_name_or_path: rombodawg/Rombos-LLM-V2.6-Qwen-14b
torch_dtype: null
attn_implementation: flash_attention_2  # As recommended for Qwen

# Data training arguments
chat_template: |
  {%- if tools %}
  {{- '<|im_start|>system\n' }}
  {%- if messages[0]['role'] == 'system' %}
      {{- messages[0]['content'] }}
  {%- else %}
      {{- 'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.' }}
  {%- endif %}
  {{- "\n\n# Tools\n\nYou may call one or more functions to assist with the user query.\n\nYou are provided with function signatures within <tools></tools> XML tags:\n<tools>" }}
  {%- for tool in tools %}
      {{- "\n" }}
      {{- tool | tojson }}
  {%- endfor %}
  {{- "\n</tools>\n\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\n<tool_call>\n{\"name\": <function-name>, \"arguments\": <args-json-object>}\n</tool_call><|im_end|>\n" }}
  {%- else %}
  {%- if messages[0]['role'] == 'system' %}
      {{- '<|im_start|>system\n' + messages[0]['content'] + '<|im_end|>\n' }}
  {%- else %}
      {{- '<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n' }}
  {%- endif %}
  {%- endif %}
  {%- for message in messages %}
  {%- if (message.role == "user") or (message.role == "system" and not loop.first) or (message.role == "assistant" and not message.tool_calls) %}
      {{- '<|im_start|>' + message.role + '\n' + message.content + '<|im_end|>' + '\n' }}
  {%- elif message.role == "assistant" %}
      {{- '<|im_start|>' + message.role }}
      {%- if message.content %}
          {{- '\n' + message.content }}
      {%- endif %}
      {%- for tool_call in message.tool_calls %}
          {%- if tool_call.function is defined %}
              {%- set tool_call = tool_call.function %}
          {%- endif %}
          {{- '\n<tool_call>\n{"name": "' }}
          {{- tool_call.name }}
          {{- '", "arguments": ' }}
          {{- tool_call.arguments | tojson }}
          {{- '}\n</tool_call>' }}
      {%- endfor %}
      {{- '<|im_end|>\n' }}
  {%- elif message.role == "tool" %}
      {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != "tool") %}
          {{- '<|im_start|>user' }}
      {%- endif %}
      {{- '\n<tool_response>\n' }}
      {{- message.content }}
      {{- '\n</tool_response>' }}
      {%- if loop.last or (messages[loop.index0 + 1].role != "tool") %}
          {{- '<|im_end|>\n' }}
      {%- endif %}
  {%- endif %}
  {%- endfor %}
  {%- if add_generation_prompt %}
      {{- '<|im_start|>assistant\n' }}
  {%- endif %}

dataset_mixer:
  arthrod/binarized_trimmed_60percent: 1.0
#arthrod/alignment-handbook/binarized_trimmed_60percent: 1.0
dataset_splits:
- train
- test
preprocessing_num_workers: 12
      

# SimPOTrainer arguments
bf16: true
beta: 2.5  # Start with 2.5 as per SimPO recommendation
gamma_beta_ratio: 0.5  # As per SimPO recommendation
do_eval: true
evaluation_strategy: steps
eval_steps: 400
gradient_accumulation_steps: 4  # Adjust to achieve total batch size close to 128
gradient_checkpointing: true
hub_model_id: rombos-llm-v2.6-qwen-14b-simpo
learning_rate: 5.0e-7  # Start with the lower end of SimPO range
log_level: info
logging_steps: 1  # As per Qwen recommendation
lr_scheduler_type: cosine
max_length: 4096  # As per Qwen recommendation
max_prompt_length: 3800
num_train_epochs: 1  # As per SimPO recommendation
optim: adamw_torch
output_dir: outputs/rombos-llm-v2.6-qwen-14b-simpo
run_name: rombos-llm-v2.6-qwen-14b-simpo
per_device_train_batch_size: 8  # Reduced due to larger model size
per_device_eval_batch_size: 8
push_to_hub: false
save_strategy: "steps"
save_steps: 1000  # As per Qwen recommendation
report_to:
- wandb
save_total_limit: 40
seed: 42
warmup_ratio: 0.1
weight_decay: 0.1  # As per Qwen recommendation
